{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "_UI2GhpW1jgZ",
        "Yrz5Jy9I2JSt",
        "d8Ujruq12M8v",
        "9M1GIPVO2PPS",
        "5lS5jK_a2S98",
        "rMAi9P2r2e2O",
        "foARt8zV2n5W"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AutoStyleGAN"
      ],
      "metadata": {
        "id": "xmC5I7gclPnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importaciones Generales"
      ],
      "metadata": {
        "id": "_UI2GhpW1jgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install munch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRMJHkrw1uDe",
        "outputId": "6897eb4d-62fb-4a99-8d02-951b1bdb4199"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting munch\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Installing collected packages: munch\n",
            "Successfully installed munch-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Munch es una pequeña librería de Python que sirve para convertir diccionarios (dict) en objetos que se pueden usar como si fueran atributos."
      ],
      "metadata": {
        "id": "K8k0wS8u2Dhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import copy\n",
        "import argparse\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "from munch import Munch\n",
        "from collections import OrderedDict\n",
        "from scipy import linalg\n",
        "import wandb\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "import cv2\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "i5lIUT5s1ZYy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilidades y Funciones Auxiliares"
      ],
      "metadata": {
        "id": "Yrz5Jy9I2JSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def he_init(module):\n",
        "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "        nn.init.kaiming_normal_(module.weight)\n",
        "        if module.bias is not None:\n",
        "            nn.init.constant_(module.bias, 0)"
      ],
      "metadata": {
        "id": "qn3X_MMq6a8K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función `he_init(module)` **inicializa** los pesos de una capa de red neuronal en PyTorch utilizando la **inicialización de He** (ideal para activaciones tipo ReLU) si el módulo es una capa `Conv2d` o `Linear`; además, si la capa tiene un sesgo (`bias`), lo inicializa en **cero**.  \n",
        "Su propósito es preparar adecuadamente las capas para que el entrenamiento del modelo sea **más rápido y estable**."
      ],
      "metadata": {
        "id": "ZJSuIVQd6uiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La **inicialización de He** es un método para asignar valores iniciales a los pesos de una red neuronal, diseñado para redes profundas que usan funciones de activación tipo **ReLU**.\n",
        "\n",
        "Busca mantener la **varianza estable** entre capas, evitando que los gradientes desaparezcan o exploten durante el entrenamiento, y con ello mejora la velocidad y estabilidad del aprendizaje."
      ],
      "metadata": {
        "id": "QOvdmSZP6eHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def denormalize(x):\n",
        "    out = (x + 1) / 2\n",
        "    return out.clamp_(0, 1)"
      ],
      "metadata": {
        "id": "wrQ3_0v07kPw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función convierte un tensor normalizado de `[-1, 1]` a `[0, 1]`, asegurando que todos los valores queden dentro del rango correcto para visualización o guardado."
      ],
      "metadata": {
        "id": "NhH_HVzD7nBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label2onehot(labels, dim):\n",
        "    batch_size = labels.size(0)\n",
        "    out = torch.zeros(batch_size, dim).to(labels.device)\n",
        "    out.scatter_(1, labels.unsqueeze(1), 1)\n",
        "    return out"
      ],
      "metadata": {
        "id": "o6ntgl0r8CoE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función `label2onehot(labels, dim)` convierte un vector de etiquetas en su representación **one-hot** en PyTorch, asegurando que los datos estén en el mismo dispositivo (CPU o GPU).\n"
      ],
      "metadata": {
        "id": "qIAp2k3t8FJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize(x, size):\n",
        "    return F.interpolate(x, size=size, mode='bilinear', align_corners=True)"
      ],
      "metadata": {
        "id": "9rI1N2SQDRQr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función `resize(x, size)` cambia el tamaño de un tensor de imagen usando interpolación bilineal, asegurando que los bordes estén alineados."
      ],
      "metadata": {
        "id": "fg2ljuBYEJ2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_image_grid(x, nrow=8, padding=2):\n",
        "    x = denormalize(x)\n",
        "    grid = make_grid(x, nrow=nrow, padding=padding)\n",
        "    return grid"
      ],
      "metadata": {
        "id": "yC3JcIpFEaes"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función `make_image_grid(x, nrow=8, padding=2)` desnormaliza un lote de imágenes y organiza varias de ellas en una rejilla o mosaico ordenado para fácil visualización o guardado.\n"
      ],
      "metadata": {
        "id": "qmbMAyyuFFVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accumulate(model1, model2, decay=0.999):\n",
        "    par1 = dict(model1.named_parameters())\n",
        "    par2 = dict(model2.named_parameters())\n",
        "\n",
        "    for k in par1.keys():\n",
        "        par1[k].data.mul_(decay).add_(par2[k].data, alpha=1 - decay)"
      ],
      "metadata": {
        "id": "3mj9FJvaFIqZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función `accumulate(model1, model2, decay)` actualiza los parámetros de `model1` como un **promedio exponencial** entre su valor actual y el valor de `model2`, para hacer que `model1` evolucione suavemente hacia `model2` y aumentar su estabilidad."
      ],
      "metadata": {
        "id": "Y5ZIrk9BFyg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MovingAverage:\n",
        "    def __init__(self, decay=0.999):\n",
        "        self.decay = decay\n",
        "        self.ema = None\n",
        "\n",
        "    def update(self, x):\n",
        "        if self.ema is None:\n",
        "            self.ema = x\n",
        "        else:\n",
        "            self.ema = self.decay * self.ema + (1 - self.decay) * x\n",
        "        return self.ema"
      ],
      "metadata": {
        "id": "zpIxZxE2GKUm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase `MovingAverage` calcula un **promedio móvil exponencial**, donde cada nuevo valor se combina de manera suavizada con el promedio anterior, permitiendo seguir las tendencias de los datos de forma estable y controlada por el parámetro `decay`."
      ],
      "metadata": {
        "id": "CqrMfp6CGq6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición de Modelos"
      ],
      "metadata": {
        "id": "d8Ujruq12M8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlk(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, normalize=False, downsample=False):\n",
        "        super().__init__()\n",
        "        self.normalize = normalize\n",
        "        self.downsample = downsample\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(dim_in, dim_out, 3, 1, 1),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True) if normalize else nn.Identity(),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(dim_out, dim_out, 3, 1, 1),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True) if normalize else nn.Identity()\n",
        "        )\n",
        "        self.downsample_layer = nn.AvgPool2d(2) if downsample else nn.Identity()\n",
        "        self.skip = nn.Conv2d(dim_in, dim_out, 1, 1, 0, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.main(x)\n",
        "        out = self.downsample_layer(out)\n",
        "        skip = self.skip(x)\n",
        "        skip = self.downsample_layer(skip)\n",
        "        return (out + skip) / math.sqrt(2)"
      ],
      "metadata": {
        "id": "CTZ9YOttHx-A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ResBlk` es un bloque residual que procesa una imagen con convoluciones y normalizaciones, opcionalmente reduce su tamaño, y combina su salida con una versión adaptada de la entrada original para **mejorar el flujo de información** y **evitar pérdida de gradientes** en redes profundas."
      ],
      "metadata": {
        "id": "m1ZOT0GPI8j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaIN(nn.Module):\n",
        "    def __init__(self, num_features, style_dim):\n",
        "        super(AdaIN, self).__init__()\n",
        "        self.fc = nn.Linear(style_dim, num_features * 2)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        h = self.fc(s)\n",
        "        gamma, beta = torch.chunk(h, chunks=2, dim=1)\n",
        "        gamma = gamma.unsqueeze(2).unsqueeze(3)\n",
        "        beta = beta.unsqueeze(2).unsqueeze(3)\n",
        "        return (1 + gamma) * x + beta"
      ],
      "metadata": {
        "id": "pfU0kUz-JqQj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase `AdaIN` ajusta la imagen `x` **canal por canal** usando parámetros (`gamma` y `beta`) que se derivan de un **vector de estilo** `s`, permitiendo cambiar dinámicamente el **estilo** de una imagen.\n"
      ],
      "metadata": {
        "id": "8O1sWGk-KnBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdainResBlk(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, style_dim=64, w_hpf=1, upsample=False):\n",
        "        super().__init__()\n",
        "        self.upsample = upsample\n",
        "        self.w_hpf = w_hpf\n",
        "\n",
        "        self.norm1 = AdaIN(dim_in, style_dim)\n",
        "        self.norm2 = AdaIN(dim_out, style_dim)\n",
        "        self.actv = nn.LeakyReLU(0.2)\n",
        "        self.conv1 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(dim_out, dim_out, 3, 1, 1)\n",
        "\n",
        "        if dim_in != dim_out:\n",
        "            self.skip = nn.Conv2d(dim_in, dim_out, 1, 1, 0)\n",
        "        else:\n",
        "            self.skip = nn.Identity()\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        x_orig = x\n",
        "\n",
        "        if self.upsample:\n",
        "            x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "            x_orig = F.interpolate(x_orig, scale_factor=2, mode='nearest')\n",
        "\n",
        "        h = self.norm1(x, s)\n",
        "        h = self.actv(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        h = self.norm2(h, s)\n",
        "        h = self.actv(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        skip = self.skip(x_orig)\n",
        "\n",
        "        out = (h + skip) / math.sqrt(2)\n",
        "        return out"
      ],
      "metadata": {
        "id": "8-vptVRmKyNr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`AdainResBlk` es un **bloque residual** que realiza procesamiento convolucional, cambia el estilo de la imagen usando **AdaIN** basado en un vector de estilo, y puede **subir la resolución** si se necesita, manteniendo estable la información mediante una **suma residual**.\n"
      ],
      "metadata": {
        "id": "2SNA0z6FLsi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_size=256, style_dim=64, max_conv_dim=512):\n",
        "        super().__init__()\n",
        "        dim_in = 64\n",
        "        blocks = []\n",
        "        blocks += [nn.Conv2d(3, dim_in, 3, 1, 1)]\n",
        "        repeat_num = int(np.log2(img_size)) - 4\n",
        "        for _ in range(repeat_num):\n",
        "            dim_out = min(dim_in*2, max_conv_dim)\n",
        "            blocks += [ResBlk(dim_in, dim_out, normalize=True, downsample=True)]\n",
        "            dim_in = dim_out\n",
        "        self.encode = nn.Sequential(*blocks)\n",
        "\n",
        "        self.decode = nn.ModuleList()\n",
        "        for _ in range(repeat_num):\n",
        "            dim_out = dim_in // 2\n",
        "            self.decode += [AdainResBlk(dim_in, dim_out, style_dim, upsample=True)]\n",
        "            dim_in = dim_out\n",
        "        self.to_rgb = nn.Sequential(\n",
        "            nn.InstanceNorm2d(dim_in, affine=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(dim_in, 3, 1, 1, 0)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        x = self.encode(x)\n",
        "        for block in self.decode:\n",
        "            x = block(x, s)\n",
        "        out = self.to_rgb(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "iK5_cDrYNGzt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El `Generator` toma una imagen y un vector de estilo, **codifica** la imagen para extraer características, **inyecta** el estilo mientras va **aumentando la resolución**, y finalmente genera una **imagen estilizada** en formato RGB.\n",
        "\n"
      ],
      "metadata": {
        "id": "hx-9tIhMNIAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, latent_dim=16, style_dim=64, num_domains=2, hidden_dim=512):\n",
        "        super(MappingNetwork, self).__init__()\n",
        "        layers = [\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        ]\n",
        "        for _ in range(3):\n",
        "            layers += [\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU()\n",
        "            ]\n",
        "        self.shared = nn.Sequential(*layers)\n",
        "\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Linear(hidden_dim, style_dim))\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        h = self.shared(z)\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out.append(layer(h))\n",
        "        out = torch.stack(out, dim=1)\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        s = out[idx, y]\n",
        "        return s"
      ],
      "metadata": {
        "id": "aWIQGzFpOg3L"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`MappingNetwork` convierte un vector latente aleatorio `z` en un **vector de estilo** `s` adaptado al **dominio deseado** `y`, utilizando una parte compartida para el procesamiento general y partes independientes para especializar cada dominio.\n"
      ],
      "metadata": {
        "id": "-Ifu9_GYOh4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleEncoder(nn.Module):\n",
        "    def __init__(self, img_size=256, style_dim=64, num_domains=2, max_conv_dim=512):\n",
        "        super().__init__()\n",
        "        dim_in = 64\n",
        "        blocks = []\n",
        "        blocks += [nn.Conv2d(3, dim_in, 3, 1, 1)]\n",
        "        repeat_num = int(np.log2(img_size)) - 2\n",
        "\n",
        "        for _ in range(repeat_num):\n",
        "            dim_out = min(dim_in*2, max_conv_dim)\n",
        "            blocks += [ResBlk(dim_in, dim_out, normalize=True, downsample=True)]\n",
        "            dim_in = dim_out\n",
        "\n",
        "        blocks += [nn.LeakyReLU(0.2)]\n",
        "        self.shared = nn.Sequential(*blocks)\n",
        "\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared += [nn.Linear(dim_in, style_dim)]\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        h = self.shared(x)\n",
        "        h = F.adaptive_avg_pool2d(h, (1,1))\n",
        "        h = h.view(h.size(0), -1)\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1)\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        s = out[idx, y]\n",
        "        return s\n"
      ],
      "metadata": {
        "id": "EDHx9rtIPytp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`StyleEncoder` convierte una imagen `x` en un **vector de estilo** `s`, adaptado al **dominio especificado** `y`, usando una codificación convolucional profunda seguida de una red específica para cada dominio."
      ],
      "metadata": {
        "id": "gi-hODbWRFr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_size=256, num_domains=3, max_conv_dim=512):\n",
        "        super().__init__()\n",
        "        dim_in = 64\n",
        "        blocks = []\n",
        "        blocks += [nn.Conv2d(3, dim_in, 3, 1, 1)]\n",
        "        repeat_num = int(np.log2(img_size)) - 2\n",
        "        for _ in range(repeat_num):\n",
        "            dim_out = min(dim_in*2, max_conv_dim)\n",
        "            blocks += [ResBlk(dim_in, dim_out, normalize=False, downsample=True)]\n",
        "            dim_in = dim_out\n",
        "        self.main = nn.Sequential(*blocks)\n",
        "        self.conv1 = nn.Conv2d(dim_out, num_domains, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        h = self.main(x)\n",
        "        out = self.conv1(h)\n",
        "        out = out.mean([2, 3])\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        out = out[idx, y]\n",
        "        return out"
      ],
      "metadata": {
        "id": "ge3_funLROPA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El `Discriminator` toma una imagen `x` y un dominio `y`, extrae características profundas, y devuelve una evaluación sobre **qué tan real** es la imagen **en ese dominio específico**.\n"
      ],
      "metadata": {
        "id": "j6w9c6LVRQZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones de Pérdida"
      ],
      "metadata": {
        "id": "9M1GIPVO2PPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adversarial_loss(logits, target):\n",
        "    targets = torch.full_like(logits, fill_value=target)\n",
        "    loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ukSimtdLVAJ3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función `adversarial_loss` calcula la **pérdida de entropía cruzada binaria** entre los logits de predicción y un objetivo (`target`=0 o 1).\n"
      ],
      "metadata": {
        "id": "57M5JesVVEWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def r1_reg(d_out, x_in):\n",
        "    batch_size = x_in.size(0)\n",
        "    grad_dout = torch.autograd.grad(\n",
        "        outputs=d_out.sum(),\n",
        "        inputs=x_in,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "    grad_dout2 = grad_dout.pow(2)\n",
        "    assert(grad_dout2.size() == x_in.size())\n",
        "    reg = grad_dout2.view(batch_size, -1).sum(1).mean(0)\n",
        "    return reg"
      ],
      "metadata": {
        "id": "uNcnfZrFV1m1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`r1_reg` calcula la **penalización R1**, que mide **qué tan sensible es el Discriminador** respecto a pequeñas perturbaciones en las imágenes reales, y **penaliza** gradientes grandes para hacer el modelo más **estable y robusto**."
      ],
      "metadata": {
        "id": "xWdOUsnpV2l9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargador de Datos"
      ],
      "metadata": {
        "id": "5lS5jK_a2S98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageFolder(Dataset):\n",
        "    def __init__(self, root, transform, mode, which='source'):\n",
        "        self.transform = transform\n",
        "        self.paths = []\n",
        "        domains = sorted(os.listdir(root))\n",
        "        for domain in domains:\n",
        "            if os.path.isdir(os.path.join(root, domain)):\n",
        "                files = os.listdir(os.path.join(root, domain))\n",
        "                files = [os.path.join(root, domain, f) for f in files]\n",
        "                self.paths += [(f, domains.index(domain)) for f in files]\n",
        "        if mode == 'train' and which == 'reference':\n",
        "            random.shuffle(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, label = self.paths[index]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        return self.transform(img), label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)"
      ],
      "metadata": {
        "id": "sCumhE0hX8B4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase `ImageFolder` carga imágenes desde carpetas organizadas por dominio, aplica transformaciones y devuelve pares (imagen transformada, etiqueta de dominio)."
      ],
      "metadata": {
        "id": "xscrOgfGX-0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transform(img_size, mode='train', prob=0.5):\n",
        "    transform = []\n",
        "\n",
        "    transform.append(transforms.Resize((img_size, img_size)))\n",
        "\n",
        "    if mode == 'train':\n",
        "        transform.append(transforms.RandomHorizontalFlip())\n",
        "        transform.append(transforms.RandomApply([\n",
        "            transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0))\n",
        "        ], p=prob))\n",
        "\n",
        "    transform.append(transforms.ToTensor())\n",
        "    transform.append(transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                           std=[0.5, 0.5, 0.5]))\n",
        "    return transforms.Compose(transform)"
      ],
      "metadata": {
        "id": "5nJ6Vqg5ZOa3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`get_transform` genera una **secuencia de transformaciones de preprocesamiento y augmentación** de imágenes para entrenamiento o validación, incluyendo cambios aleatorios, redimensionamiento, conversión a tensor y normalización.\n"
      ],
      "metadata": {
        "id": "XKWvz67BZSHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_loader(root, which='source', img_size=256, batch_size=8, prob=0.5, num_workers=4):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.RandomHorizontalFlip(p=prob),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    dataset = ImageFolder(root=root, transform=transform, mode=which)\n",
        "    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "\n",
        "    return loader"
      ],
      "metadata": {
        "id": "PSNX9L5taH9R"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`get_train_loader` crea un `DataLoader` para cargar **batches de imágenes preprocesadas** y **aumentadas**, listas para ser usadas durante el entrenamiento.\n"
      ],
      "metadata": {
        "id": "KQrlBX9oaJHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_loader(root, img_size=256, batch_size=8, shuffle=False, num_workers=4, mode='reference'):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    dataset = ImageFolder(root=root, transform=transform, mode=mode)\n",
        "    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, drop_last=False)\n",
        "\n",
        "    return loader"
      ],
      "metadata": {
        "id": "icfrPQQya9SF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`get_test_loader` crea un `DataLoader` que carga imágenes preprocesadas para la fase de **prueba o evaluación**, **sin augmentaciones**, y las organiza en batches.\n"
      ],
      "metadata": {
        "id": "2crKdKoIbAJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Métricas de Evaluación"
      ],
      "metadata": {
        "id": "rMAi9P2r2e2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionV3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        inception = models.inception_v3(pretrained=True, transform_input=False)\n",
        "        self.blocks = nn.Sequential(\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            inception.Conv2d_3b_1x1,\n",
        "            inception.Conv2d_4a_3x3,\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            inception.Mixed_5b,\n",
        "            inception.Mixed_5c,\n",
        "            inception.Mixed_5d,\n",
        "            inception.Mixed_6a,\n",
        "            inception.Mixed_6b,\n",
        "            inception.Mixed_6c,\n",
        "            inception.Mixed_6d,\n",
        "            inception.Mixed_6e\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.blocks(x)"
      ],
      "metadata": {
        "id": "9kUREsuRdhCM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`InceptionV3` extrae **representaciones profundas** de imágenes usando una **parte de la red Inception v3 preentrenada**."
      ],
      "metadata": {
        "id": "JHcQstmodiTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_fid(real_features, fake_features):\n",
        "    mu1 = np.mean(real_features, axis=0)\n",
        "    mu2 = np.mean(fake_features, axis=0)\n",
        "    sigma1 = np.cov(real_features, rowvar=False)\n",
        "    sigma2 = np.cov(fake_features, rowvar=False)\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    fid = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2*np.trace(covmean)\n",
        "    return fid"
      ],
      "metadata": {
        "id": "y4agcGLGd4wh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función `calculate_fid` compara las distribuciones de características de imágenes reales y generadas calculando la **Fréchet Inception Distance (FID)."
      ],
      "metadata": {
        "id": "oc1vB8Nod7jU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LPIPS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vgg = models.vgg16(pretrained=True).features[:16].eval()\n",
        "        for param in self.vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x_vgg = self.vgg(x)\n",
        "        y_vgg = self.vgg(y)\n",
        "        return F.l1_loss(x_vgg, y_vgg)"
      ],
      "metadata": {
        "id": "pakMP9BvevKc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase `LPIPS` mide la **distancia perceptual** entre dos imágenes, **comparando sus características internas** extraídas de las primeras capas de una red **VGG16 preentrenada**, y usando una **pérdida L1** sobre esas características.\n"
      ],
      "metadata": {
        "id": "KQ1QSY8Bfes3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición de Solver"
      ],
      "metadata": {
        "id": "poHvZdQJ2cTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Solver(object):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.G = Generator(args.img_size, args.style_dim)\n",
        "        self.D = Discriminator(args.img_size, args.num_domains)\n",
        "        self.M = MappingNetwork(args.latent_dim, args.style_dim, args.num_domains)\n",
        "        self.S = StyleEncoder(args.img_size, args.style_dim, args.num_domains)\n",
        "\n",
        "        self.G.to(self.device)\n",
        "        self.D.to(self.device)\n",
        "        self.M.to(self.device)\n",
        "        self.S.to(self.device)\n",
        "\n",
        "        self.g_optimizer = torch.optim.Adam(\n",
        "            list(self.G.parameters()) + list(self.M.parameters()) + list(self.S.parameters()),\n",
        "            args.lr, [args.beta1, args.beta2], weight_decay=args.weight_decay)\n",
        "        self.d_optimizer = torch.optim.Adam(self.D.parameters(),\n",
        "            args.lr, [args.beta1, args.beta2], weight_decay=args.weight_decay)\n",
        "\n",
        "        self.start_iter = 0\n",
        "        self.inception = InceptionV3().to(self.device)\n",
        "        self.lpips = LPIPS().to(self.device)\n",
        "\n",
        "        if self.args.resume_iter > 0:\n",
        "            ckpt_path = os.path.join(self.args.checkpoint_dir, f'{self.args.resume_iter}_nets_ema.ckpt')\n",
        "            if os.path.exists(ckpt_path):\n",
        "                print(f\"Cargando checkpoint desde {ckpt_path}...\")\n",
        "                checkpoint = torch.load(ckpt_path, map_location=self.device)\n",
        "\n",
        "                self.G.load_state_dict(checkpoint['generator'])\n",
        "                self.M.load_state_dict(checkpoint['mapping_network'])\n",
        "                self.S.load_state_dict(checkpoint['style_encoder'])\n",
        "                self.D.load_state_dict(checkpoint['discriminator'])\n",
        "\n",
        "                self.start_iter = self.args.resume_iter\n",
        "                print(f\"Checkpoint cargado correctamente. Empezando desde iteración {self.start_iter}.\")\n",
        "            else:\n",
        "                print(f\"No se encontró checkpoint en {ckpt_path}. Entrenando desde cero.\")\n",
        "        else:\n",
        "            print(\"Entrenando desde cero (resume_iter = 0).\")\n",
        "\n",
        "    def _reset_grad(self):\n",
        "        self.g_optimizer.zero_grad()\n",
        "        self.d_optimizer.zero_grad()\n",
        "\n",
        "    def _compute_d_loss(self, x_real, y_org, y_trg, z_trg, x_ref, masks=None):\n",
        "        x_real.requires_grad_()\n",
        "        out_real = self.D(x_real, y_org)\n",
        "        loss_real = adversarial_loss(out_real, 1)\n",
        "        loss_reg = r1_reg(out_real, x_real)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            s_trg = self.M(z_trg, y_trg)\n",
        "            x_fake = self.G(x_real, s_trg)\n",
        "        out_fake = self.D(x_fake, y_trg)\n",
        "        loss_fake = adversarial_loss(out_fake, 0)\n",
        "\n",
        "        loss = loss_real + loss_fake + self.args.lambda_reg * loss_reg\n",
        "        return loss\n",
        "\n",
        "    def _compute_g_loss(self, x_real, y_org, y_trg, z_trgs, x_refs, masks=None):\n",
        "        s_trg = self.M(z_trgs[0], y_trg)\n",
        "        x_fake = self.G(x_real, s_trg)\n",
        "        out_fake = self.D(x_fake, y_trg)\n",
        "        loss_adv = adversarial_loss(out_fake, 1)\n",
        "\n",
        "        s_pred = self.S(x_fake, y_trg)\n",
        "        loss_sty = F.l1_loss(s_pred, s_trg)\n",
        "\n",
        "        x_rec = self.G(x_fake, self.S(x_real, y_org))\n",
        "        loss_cyc = F.l1_loss(x_rec, x_real)\n",
        "\n",
        "        s_trg2 = self.M(z_trgs[1], y_trg)\n",
        "        x_fake2 = self.G(x_real, s_trg2)\n",
        "        loss_ds = -F.l1_loss(x_fake, x_fake2)\n",
        "\n",
        "        loss = loss_adv + self.args.lambda_sty * loss_sty + self.args.lambda_cyc * loss_cyc\n",
        "        if self.args.lambda_ds > 0:\n",
        "            loss += self.args.lambda_ds * loss_ds\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # Entrenamiento\n",
        "    def train(self, loaders):\n",
        "      src_loader = loaders.src\n",
        "      ref_loader = loaders.ref\n",
        "      print(\"Training started...\")\n",
        "\n",
        "\n",
        "      wandb.init(\n",
        "          project=\"AutoStyleGAN\",\n",
        "          name=f\"Entrenamiento_2\",\n",
        "          config=vars(self.args)\n",
        "      )\n",
        "\n",
        "      for it in range(self.start_iter, self.args.total_iters):\n",
        "          x_real, y_org = next(iter(src_loader))\n",
        "          x_ref, y_trg = next(iter(ref_loader))\n",
        "          x_real, y_org = x_real.to(self.device), y_org.to(self.device)\n",
        "          x_ref, y_trg = x_ref.to(self.device), y_trg.to(self.device)\n",
        "\n",
        "          # Entrenamiento del discriminador\n",
        "          z_trg = torch.randn(x_real.size(0), self.args.latent_dim).to(self.device)\n",
        "          z_trgs = [torch.randn(x_real.size(0), self.args.latent_dim).to(self.device) for _ in range(2)]\n",
        "\n",
        "          d_loss = self._compute_d_loss(x_real, y_org, y_trg, z_trg, x_ref)\n",
        "          self._reset_grad()\n",
        "          d_loss.backward()\n",
        "          self.d_optimizer.step()\n",
        "\n",
        "          # Entrenamiento del generador\n",
        "          g_loss = self._compute_g_loss(x_real, y_org, y_trg, z_trgs, x_ref)\n",
        "          self._reset_grad()\n",
        "          g_loss.backward()\n",
        "          self.g_optimizer.step()\n",
        "\n",
        "          if (it + 1) % self.args.print_every == 0:\n",
        "              print(f\"Iter [{it+1}/{self.args.total_iters}] d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}\")\n",
        "\n",
        "              wandb.log({\n",
        "                  \"Discriminator Loss\": d_loss.item(),\n",
        "                  \"Generator Loss\": g_loss.item(),\n",
        "                  \"Iteration\": it + 1\n",
        "              })\n",
        "\n",
        "\n",
        "          if (it + 1) % self.args.save_every == 0:\n",
        "\n",
        "              ckpt_path = os.path.join(self.args.checkpoint_dir, f'{it+1}_nets_ema.ckpt')\n",
        "              torch.save({\n",
        "                  'generator': self.G.state_dict(),\n",
        "                  'mapping_network': self.M.state_dict(),\n",
        "                  'style_encoder': self.S.state_dict(),\n",
        "                  'discriminator': self.D.state_dict()\n",
        "              }, ckpt_path)\n",
        "              print(f\"Checkpoint guardado en {ckpt_path}\")\n",
        "\n",
        "              self.G.eval()\n",
        "              self.S.eval()\n",
        "              with torch.no_grad():\n",
        "                  x_src, y_src = next(iter(src_loader))\n",
        "                  x_ref, y_ref = next(iter(ref_loader))\n",
        "                  x_src, y_src = x_src.to(self.device), y_src.to(self.device)\n",
        "                  x_ref, y_ref = x_ref.to(self.device), y_ref.to(self.device)\n",
        "\n",
        "                  s_ref = self.S(x_ref, y_ref)\n",
        "                  x_fake = self.G(x_src, s_ref)\n",
        "                  x_concat = torch.cat([x_src, x_ref, x_fake], dim=3)\n",
        "\n",
        "                  os.makedirs(self.args.sample_dir, exist_ok=True)\n",
        "\n",
        "                  sample_path = os.path.join(self.args.sample_dir, f'sample_{it+1}.png')\n",
        "                  save_image((x_concat.data.cpu() + 1) / 2, sample_path, nrow=1, padding=0)\n",
        "                  print(f\"Muestra guardada en {sample_path}\")\n",
        "\n",
        "                  wandb.log({\n",
        "                      f\"Samples_{it+1}\": wandb.Image(sample_path)\n",
        "                  })\n",
        "              self.G.train()\n",
        "              self.S.train()\n",
        "\n",
        "\n",
        "    def sample(self, loaders):\n",
        "      src_loader = loaders.src\n",
        "      ref_loader = loaders.ref\n",
        "      print(\"Sampling started...\")\n",
        "\n",
        "      x_src, y_src = next(iter(src_loader))\n",
        "      x_ref, y_ref = next(iter(ref_loader))\n",
        "\n",
        "      n_samples = min(x_src.size(0), x_ref.size(0))\n",
        "\n",
        "      x_src, y_src = x_src[:n_samples].to(self.device), y_src[:n_samples].to(self.device)\n",
        "      x_ref, y_ref = x_ref[:n_samples].to(self.device), y_ref[:n_samples].to(self.device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          s_ref = self.S(x_ref, y_ref)\n",
        "          x_fake = self.G(x_src, s_ref)\n",
        "          x_concat = torch.cat([x_src, x_ref, x_fake], dim=3)\n",
        "\n",
        "          save_path = os.path.join(self.args.sample_dir, 'sample.png')\n",
        "          os.makedirs(self.args.sample_dir, exist_ok=True)\n",
        "          save_image(denormalize(x_concat.data.cpu()), save_path, nrow=1, padding=0, normalize=True)\n",
        "\n",
        "      print(f\"Sample images saved to {save_path}\")\n",
        "\n",
        "\n",
        "    def evaluate(self):\n",
        "        print(\"Evaluation started...\")\n",
        "        real_images = torch.randn(50, 3, 256, 256).to(self.device)\n",
        "        fake_images = torch.randn(50, 3, 256, 256).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            real_feats = self.inception(resize(real_images, size=(299,299))).view(50, -1).cpu().numpy()\n",
        "            fake_feats = self.inception(resize(fake_images, size=(299,299))).view(50, -1).cpu().numpy()\n",
        "\n",
        "        fid_score = calculate_fid(real_feats, fake_feats)\n",
        "        lpips_score = self.lpips(real_images, fake_images).mean().item()\n",
        "\n",
        "        print(f\"FID: {fid_score:.4f}, LPIPS: {lpips_score:.4f}\")"
      ],
      "metadata": {
        "id": "SnzXo8lihmTa"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase `Solver` es el **controlador** del entrenamiento, generación y evaluación del GAN,  \n",
        "encargándose de calcular pérdidas, actualizar parámetros, crear muestras, y evaluar la calidad de las imágenes generadas.\n"
      ],
      "metadata": {
        "id": "MK_QrTM_jAxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código Principal"
      ],
      "metadata": {
        "id": "foARt8zV2n5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def str2bool(v):\n",
        "    return v.lower() in ('true')"
      ],
      "metadata": {
        "id": "XxD6AJch2qej"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función convierte una cadena de texto (v) en un valor booleano (True o False),\n",
        "basándose en si el texto es 'true' (sin distinguir mayúsculas o minúsculas)."
      ],
      "metadata": {
        "id": "WjphMBtm3GME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subdirs(dname):\n",
        "    return [d for d in os.listdir(dname)\n",
        "            if os.path.isdir(os.path.join(dname, d))]"
      ],
      "metadata": {
        "id": "D8jlcE0g29gA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta función te devuelve una lista con todos los subdirectorios (carpetas) que hay dentro de una carpeta principal."
      ],
      "metadata": {
        "id": "4ALEc_NZ2zG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    print(args)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    solver = Solver(args)\n",
        "\n",
        "    if args.mode == 'train':\n",
        "        assert len(subdirs(args.train_img_dir)) == args.num_domains\n",
        "        assert len(subdirs(args.val_img_dir)) == args.num_domains\n",
        "\n",
        "        loaders = Munch(\n",
        "            src=get_train_loader(root=args.train_img_dir,\n",
        "                                 which='source',\n",
        "                                 img_size=args.img_size,\n",
        "                                 batch_size=args.batch_size,\n",
        "                                 prob=args.randcrop_prob,\n",
        "                                 num_workers=args.num_workers),\n",
        "            ref=get_train_loader(root=args.train_img_dir,\n",
        "                                 which='reference',\n",
        "                                 img_size=args.img_size,\n",
        "                                 batch_size=args.batch_size,\n",
        "                                 prob=args.randcrop_prob,\n",
        "                                 num_workers=args.num_workers),\n",
        "            val=get_test_loader(root=args.val_img_dir,\n",
        "                                img_size=args.img_size,\n",
        "                                batch_size=args.val_batch_size,\n",
        "                                shuffle=True,\n",
        "                                num_workers=args.num_workers)\n",
        "        )\n",
        "        solver.train(loaders)\n",
        "\n",
        "    elif args.mode == 'sample':\n",
        "        loaders = Munch(\n",
        "            src=get_test_loader(root=args.src_dir,\n",
        "                                img_size=args.img_size,\n",
        "                                batch_size=args.val_batch_size,\n",
        "                                shuffle=False,\n",
        "                                num_workers=args.num_workers),\n",
        "            ref=get_test_loader(root=args.ref_dir,\n",
        "                                img_size=args.img_size,\n",
        "                                batch_size=args.val_batch_size,\n",
        "                                shuffle=False,\n",
        "                                num_workers=args.num_workers)\n",
        "        )\n",
        "        solver.sample(loaders)\n",
        "\n",
        "    elif args.mode == 'eval':\n",
        "        solver.evaluate()\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "sNIHhW7o3Xqz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función `main(args)` es el núcleo que, dependiendo del valor de `args.mode`, controla el flujo del programa: primero imprime los argumentos, optimiza PyTorch para la GPU y fija una semilla para reproducibilidad.\n",
        "\n",
        "Luego crea un objeto `Solver` que se encarga de las acciones principales.\n",
        "\n",
        "- Si el modo es `'train'`, carga los datos de entrenamiento, validación y referencia, verifica que el número de subcarpetas coincida con los dominios esperados, y llama a `solver.train()`.\n",
        "\n",
        "- Si es `'sample'`, carga datos de prueba para generar muestras con `solver.sample()`.\n",
        "\n",
        "- Si es `'eval'`, ejecuta la evaluación del modelo con `solver.evaluate()`.\n",
        "\n",
        "- En cualquier otro caso, lanza un error indicando que el modo no está implementado."
      ],
      "metadata": {
        "id": "0lANXRvS3Ygk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración de Parámetros"
      ],
      "metadata": {
        "id": "bUkwyIA_40p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Pacolaz/AutoStyleGAN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jjazm6j_sCnn",
        "outputId": "06852427-9376-4574-8fb8-ab98f6261ca5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AutoStyleGAN'...\n",
            "remote: Enumerating objects: 958, done.\u001b[K\n",
            "remote: Counting objects: 100% (958/958), done.\u001b[K\n",
            "remote: Compressing objects: 100% (955/955), done.\u001b[K\n",
            "remote: Total 958 (delta 2), reused 949 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (958/958), 8.97 MiB | 12.36 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_root = 'AutoStyleGAN'\n",
        "\n",
        "expr_folders = [\n",
        "    'expr/checkpoints/autos',\n",
        "    'expr/eval',\n",
        "    'expr/results',\n",
        "    'expr/samples'\n",
        "]\n",
        "\n",
        "for folder in expr_folders:\n",
        "    path = os.path.join(project_root, folder)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "print(\"Carpetas creadas exitosamente\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptNSBfWBxrYo",
        "outputId": "ceb8b557-43dd-43e8-9476-35c3f32945a0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carpetas creadas exitosamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "args = SimpleNamespace(\n",
        "    img_size=128,\n",
        "    num_domains=3, # BMW, Corvette, Mazda\n",
        "    latent_dim=16,\n",
        "    hidden_dim=1024,\n",
        "    style_dim=64,\n",
        "    lambda_reg=1,\n",
        "    lambda_cyc=1,\n",
        "    lambda_sty=1,\n",
        "    lambda_ds=1,\n",
        "    ds_iter=100000,\n",
        "    randcrop_prob=0.5,\n",
        "    total_iters=100000,\n",
        "    resume_iter=24000,\n",
        "    batch_size=4,\n",
        "    val_batch_size=4,\n",
        "    lr=1e-4,\n",
        "    f_lr=1e-6,\n",
        "    beta1=0.0,\n",
        "    beta2=0.99,\n",
        "    weight_decay=1e-4,\n",
        "    num_outs_per_domain=10,\n",
        "    mode='train', # train, sample, eval\n",
        "    num_workers=4,\n",
        "    seed=8365,\n",
        "    train_img_dir = 'AutoStyleGAN/dataset/train',\n",
        "    val_img_dir = 'AutoStyleGAN/dataset/val',\n",
        "    sample_dir = 'AutoStyleGAN/expr/samples',\n",
        "    checkpoint_dir = 'AutoStyleGAN/expr/checkpoints',\n",
        "    eval_dir = 'AutoStyleGAN/expr/eval',\n",
        "    result_dir = 'AutoStyleGAN/expr/results',\n",
        "    src_dir = 'AutoStyleGAN/assets/src',\n",
        "    ref_dir = 'AutoStyleGAN/assets/ref',\n",
        "    print_every=100,\n",
        "    sample_every=500,\n",
        "    save_every=500,\n",
        "    eval_every=500\n",
        ")"
      ],
      "metadata": {
        "id": "JrBj2J6b_yEG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-G0Y_52Qhf6h",
        "outputId": "9e96186d-ca77-4493-b787-c8c9be84b5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "namespace(img_size=128, num_domains=3, latent_dim=16, hidden_dim=1024, style_dim=64, lambda_reg=1, lambda_cyc=1, lambda_sty=1, lambda_ds=1, ds_iter=100000, randcrop_prob=0.5, total_iters=100000, resume_iter=24000, batch_size=4, val_batch_size=4, lr=0.0001, f_lr=1e-06, beta1=0.0, beta2=0.99, weight_decay=0.0001, num_outs_per_domain=10, mode='train', num_workers=4, seed=8365, train_img_dir='AutoStyleGAN/dataset/train', val_img_dir='AutoStyleGAN/dataset/val', sample_dir='AutoStyleGAN/expr/samples', checkpoint_dir='AutoStyleGAN/expr/checkpoints', eval_dir='AutoStyleGAN/expr/eval', result_dir='AutoStyleGAN/expr/results', src_dir='AutoStyleGAN/assets/src', ref_dir='AutoStyleGAN/assets/ref', print_every=100, sample_every=500, save_every=500, eval_every=500)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando checkpoint desde AutoStyleGAN/expr/checkpoints/24000_nets_ema.ckpt...\n",
            "Checkpoint cargado correctamente. Empezando desde iteración 24000.\n",
            "Training started...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Entrenamiento_2</strong> at: <a href='https://wandb.ai/luisf-lopez-iteso/AutoStyleGAN/runs/jd4vwpre' target=\"_blank\">https://wandb.ai/luisf-lopez-iteso/AutoStyleGAN/runs/jd4vwpre</a><br> View project at: <a href='https://wandb.ai/luisf-lopez-iteso/AutoStyleGAN' target=\"_blank\">https://wandb.ai/luisf-lopez-iteso/AutoStyleGAN</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250429_000619-jd4vwpre/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250429_000858-kc02emlo</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/luisf-lopez-iteso/AutoStyleGAN/runs/kc02emlo' target=\"_blank\">Entrenamiento_2</a></strong> to <a href='https://wandb.ai/luisf-lopez-iteso/AutoStyleGAN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/luisf-lopez-iteso/AutoStyleGAN' target=\"_blank\">https://wandb.ai/luisf-lopez-iteso/AutoStyleGAN</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/luisf-lopez-iteso/AutoStyleGAN/runs/kc02emlo' target=\"_blank\">https://wandb.ai/luisf-lopez-iteso/AutoStyleGAN/runs/kc02emlo</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter [24100/100000] d_loss: 0.5440, g_loss: 2.9086\n",
            "Iter [24200/100000] d_loss: 0.6751, g_loss: 2.4661\n",
            "Iter [24300/100000] d_loss: 0.7235, g_loss: 2.4762\n",
            "Iter [24400/100000] d_loss: 0.7929, g_loss: 1.9892\n",
            "Iter [24500/100000] d_loss: 0.4674, g_loss: 2.5760\n",
            "Checkpoint guardado en AutoStyleGAN/expr/checkpoints/24500_nets_ema.ckpt\n",
            "Muestra guardada en AutoStyleGAN/expr/samples/sample_24500.png\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter [24600/100000] d_loss: 0.5507, g_loss: 1.9651\n",
            "Iter [24700/100000] d_loss: 0.4183, g_loss: 3.1053\n",
            "Iter [24800/100000] d_loss: 0.7290, g_loss: 1.9248\n",
            "Iter [24900/100000] d_loss: 0.6641, g_loss: 1.4705\n",
            "Iter [25000/100000] d_loss: 0.6118, g_loss: 2.5347\n",
            "Checkpoint guardado en AutoStyleGAN/expr/checkpoints/25000_nets_ema.ckpt\n",
            "Muestra guardada en AutoStyleGAN/expr/samples/sample_25000.png\n",
            "Iter [25100/100000] d_loss: 0.8215, g_loss: 1.5936\n",
            "Iter [25200/100000] d_loss: 1.1846, g_loss: 2.3469\n",
            "Iter [25300/100000] d_loss: 0.6002, g_loss: 1.9973\n",
            "Iter [25400/100000] d_loss: 0.7153, g_loss: 2.4499\n",
            "Iter [25500/100000] d_loss: 0.7628, g_loss: 3.5995\n",
            "Checkpoint guardado en AutoStyleGAN/expr/checkpoints/25500_nets_ema.ckpt\n",
            "Muestra guardada en AutoStyleGAN/expr/samples/sample_25500.png\n",
            "Iter [25600/100000] d_loss: 0.6612, g_loss: 0.6519\n",
            "Iter [25700/100000] d_loss: 0.7981, g_loss: 1.6274\n",
            "Iter [25800/100000] d_loss: 0.6842, g_loss: 3.4070\n",
            "Iter [25900/100000] d_loss: 0.4642, g_loss: 2.3215\n",
            "Iter [26000/100000] d_loss: 0.5779, g_loss: 1.6219\n",
            "Checkpoint guardado en AutoStyleGAN/expr/checkpoints/26000_nets_ema.ckpt\n",
            "Muestra guardada en AutoStyleGAN/expr/samples/sample_26000.png\n",
            "Iter [26100/100000] d_loss: 0.8264, g_loss: 1.2609\n",
            "Iter [26200/100000] d_loss: 0.4847, g_loss: 1.3992\n",
            "Iter [26300/100000] d_loss: 0.7234, g_loss: 1.6859\n",
            "Iter [26400/100000] d_loss: 0.6941, g_loss: 2.2130\n",
            "Iter [26500/100000] d_loss: 0.6180, g_loss: 1.7965\n",
            "Checkpoint guardado en AutoStyleGAN/expr/checkpoints/26500_nets_ema.ckpt\n",
            "Muestra guardada en AutoStyleGAN/expr/samples/sample_26500.png\n",
            "Iter [26600/100000] d_loss: 0.6478, g_loss: 3.1696\n",
            "Iter [26700/100000] d_loss: 0.4818, g_loss: 2.6518\n",
            "Iter [26800/100000] d_loss: 0.6361, g_loss: 2.8641\n",
            "Iter [26900/100000] d_loss: 0.5140, g_loss: 2.0287\n",
            "Iter [27000/100000] d_loss: 0.7169, g_loss: 3.4169\n",
            "Checkpoint guardado en AutoStyleGAN/expr/checkpoints/27000_nets_ema.ckpt\n",
            "Muestra guardada en AutoStyleGAN/expr/samples/sample_27000.png\n",
            "Iter [27100/100000] d_loss: 0.5210, g_loss: 2.2730\n",
            "Iter [27200/100000] d_loss: 1.0232, g_loss: 0.7834\n",
            "Iter [27300/100000] d_loss: 0.5414, g_loss: 2.3939\n",
            "Iter [27400/100000] d_loss: 0.7775, g_loss: 1.3557\n",
            "Iter [27500/100000] d_loss: 0.5436, g_loss: 2.4271\n",
            "Checkpoint guardado en AutoStyleGAN/expr/checkpoints/27500_nets_ema.ckpt\n",
            "Muestra guardada en AutoStyleGAN/expr/samples/sample_27500.png\n"
          ]
        }
      ]
    }
  ]
}